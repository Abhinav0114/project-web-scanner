import requests
from bs4 import BeautifulSoup
import re
from urllib.parse import urljoin, urlparse

XSS_PAYLOADS = [
    "\"><script>alert(1)</script>",
    "'><img src=x onerror=alert(1)>",
    '<svg/onload=alert(1)>'
]

SQLI_PAYLOADS = [
    "' OR '1'='1",
    "' OR 1=1--",
    "' UNION SELECT NULL--"
]

SQL_ERRORS = [
    "sql syntax", "mysql_fetch", "you have an error in your sql syntax",
    "unclosed quotation mark", "syntax error"
]

class Scanner:
    def __init__(self, base_url, max_depth=2):
        self.base_url = base_url.rstrip('/')
        self.max_depth = max_depth
        self.visited = set()
        self.vulnerabilities = []
        self.session = requests.Session()
        self.found_set = set()  # Deduplication set added here

    def normalize_url(self, url):
        parsed = urlparse(url)
        return f"{parsed.scheme}://{parsed.netloc}{parsed.path}"

    def checks_csrf(self, form, url):
        """Check for CSRF token presence in forms."""
        inputs = form.find_all(['input', 'textarea', 'select'])
        tokens = [inp for inp in inputs if 'csrf' in (inp.get('name') or '').lower() or 'token' in (inp.get('name') or '').lower()]
        if not tokens:
            self.report({
                'vuln_type': 'CSRF',
                'url': url,
                'param': 'N/A',
                'evidence': 'No CSRF token in form',
                'severity': 'Medium'
            })

    def find_forms(self, url):
        try:
            resp = self.session.get(url, timeout=5)
            soup = BeautifulSoup(resp.text, "html.parser")
            forms = soup.find_all("form")
            return forms, resp.text
        except Exception:
            return [], ''

    def test_xss(self, url, form):
        action = form.get('action') or url
        method = form.get('method', 'get').lower()
        inputs = form.find_all(['input', 'textarea', 'select'])
        target_url = urljoin(url, action)
        for inp in inputs:
            name = inp.get('name')
            if not name:
                continue
            for payload in XSS_PAYLOADS:
                data = {name: payload}
                try:
                    if method == 'post':
                        resp = self.session.post(target_url, data=data, timeout=5)
                    else:
                        resp = self.session.get(target_url, params=data, timeout=5)
                    if payload in resp.text:
                        self.report({
                            'vuln_type': 'XSS',
                            'url': target_url,
                            'param': name,
                            'evidence': f'Payload "{payload}" reflected in response',
                            'severity': 'High'
                        })
                except Exception:
                    continue

    def test_sqli(self, url, form):
        action = form.get('action') or url
        method = form.get('method', 'get').lower()
        inputs = form.find_all(['input', 'textarea', 'select'])
        target_url = urljoin(url, action)
        for inp in inputs:
            name = inp.get('name')
            if not name:
                continue
            for payload in SQLI_PAYLOADS:
                data = {name: payload}
                try:
                    if method == 'post':
                        resp = self.session.post(target_url, data=data, timeout=5)
                    else:
                        resp = self.session.get(target_url, params=data, timeout=5)
                    for err in SQL_ERRORS:
                        if re.search(err, resp.text, re.I):
                            self.report({
                                'vuln_type': 'SQL Injection',
                                'url': target_url,
                                'param': name,
                                'evidence': f'SQL error found for payload "{payload}"',
                                'severity': 'Critical'
                            })
                            break
                except Exception:
                    continue

    def crawl(self, url, depth=0):
        if depth > self.max_depth or url in self.visited:
            return
        self.visited.add(url)
        try:
            resp = self.session.get(url, timeout=5)
            soup = BeautifulSoup(resp.text, 'html.parser')

            forms = soup.find_all('form')
            for form in forms:
                self.checks_csrf(form, url)
                self.test_xss(url, form)
                self.test_sqli(url, form)

            links = soup.find_all('a', href=True)
            for link in links:
                next_url = urljoin(url, link['href'])
                if next_url.startswith(self.base_url):
                    norm_url = self.normalize_url(next_url)
                    if norm_url not in self.visited:
                        self.crawl(norm_url, depth + 1)
        except Exception:
            pass

    def report(self, vuln):
        key = (vuln['vuln_type'], vuln['url'], vuln['param'], vuln['evidence'])
        if key not in self.found_set:
            self.found_set.add(key)
            self.vulnerabilities.append(vuln)

    def scan(self):
        self.crawl(self.base_url)
        return self.vulnerabilities

def scan_site(url):
    scanner = Scanner(url)
    return scanner.scan()
